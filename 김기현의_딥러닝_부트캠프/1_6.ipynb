{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "행렬 곱 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2]) torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor([[1,2],\n",
    "                       [3,4],\n",
    "                       [4,5]])\n",
    "\n",
    "y = torch.FloatTensor([[1,2],\n",
    "                       [1,2]])\n",
    "\n",
    "print(x.size(), y.size())\n",
    "\n",
    "z = torch.matmul(x,y)\n",
    "print(z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor(3, 3, 2)\n",
    "y = torch.FloatTensor(3, 2, 3)\n",
    "z = torch.bmm(x, y)\n",
    "print(z.size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "선형 계층 : 행렬 곱셈과 벡터의 덧셈으로 이루어져 있기 때문에 선형변환 이라 볼 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "# 선형 계층 구현\n",
    "\n",
    "# 3*2 크기의 행렬 W\n",
    "W = torch.FloatTensor([[1,2],\n",
    "                       [3,4],\n",
    "                       [5,6]])\n",
    "\n",
    "# 2개의 요소를 갖는 벡터 b\n",
    "b = torch.FloatTensor([2,2])\n",
    "\n",
    "def linear(x, W, b):\n",
    "    y = torch.matmul(x, W)+b\n",
    "    \n",
    "    return y\n",
    "\n",
    "x = torch.FloatTensor(4,3)\n",
    "y = linear(x, W, b)\n",
    "print(y.size())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.nn.Module 클래스 상속 받기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Module 을 상속받은 클래스는 보통 __init__ 과 forward 사용\n",
    "# __init__ : 계층 내부에서 필요한 변수를 미리 선언 + 또 다른 계층을 소유하도록 할 수 있음\n",
    "# forward :  계층을 통과하는데 필요한 계산 수행 \n",
    "\n",
    "import torch.nn as nn \n",
    "\n",
    "class MyLinear(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=3, output_dim=2):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.W = torch.FloatTensor(input_dim, output_dim)\n",
    "        self.b = torch.FloatTensor(output_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # |x| = (batch_size, input_dim)\n",
    "        y = torch.matmul(x, self.W) + self.b\n",
    "        # |y| = (batch_size, input_dim) * (input_dim, output_dim)\n",
    "\n",
    "        return y\n",
    "\n",
    "linear = MyLinear(3,2)\n",
    "y = linear(x)\n",
    "\n",
    "# 아직 제대로 된 모델이 아님( 학습할 수 있는 파라미터가 없음 ) \n",
    "for p in linear.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "올바른 방법:nn.Parameter 활용하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.1555, -0.3334, -0.5157],\n",
      "        [-0.3579,  0.1233, -0.3723]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1968,  0.0085], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "class MyLinear(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=3, output_dim=2):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        # 학습이 가능하도록 torch.nn.Parameter 클래스 활용 \n",
    "        self.W = nn.Parameter(torch.FloatTensor(input_dim, output_dim))\n",
    "        self.b = nn.Parameter(torch.FloatTensor(output_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # |x| = (batch_size, input_dim)\n",
    "        y = torch.matmul(x, self.W) +self.b\n",
    "        # |y| = (batch_size, input_dim) * (input_dim, output_dim)\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "for p in linear.parameters():\n",
    "    print(p)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Linear 활용하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.2484,  0.2972, -0.4306],\n",
      "        [-0.1090, -0.3679, -0.2190]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.2391, 0.3523], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "linear = nn.Linear(3,2)\n",
    "y = linear(x)\n",
    "\n",
    "for p in linear.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinear(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=3, output_dim=2):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # __init__ 함수 내부에 nn.Linear을 선언하여 self.linear에 저장 \n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # |x| = (batch_size, input_dim)\n",
    "        y = self.linear(x)\n",
    "        # |y| = (batch_size, input_dim) * (input_dim, output_dim)\n",
    "\n",
    "        return y\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU 사용하기 - Cuda 함수 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.3694e-38, 3.8177e-05],\n",
      "        [0.0000e+00, 5.1569e-01]], device='cuda:0')\n",
      "====================================================================================================\n",
      "tensor([[1.4013e-45, 1.5549e-01],\n",
      "        [1.4013e-45, 0.0000e+00]])\n",
      "tensor([[1.4013e-45, 1.5549e-01],\n",
      "        [1.4013e-45, 0.0000e+00]], device='cuda:0')\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Linear(in_features=2, out_features=2, bias=True)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPU의 메모리 상에 텐서 생성 \n",
    "x = torch.cuda.FloatTensor(2, 2)\n",
    "print(x)\n",
    "\n",
    "print('='*100)\n",
    "# cuda 함수를 통해 CPU 메모리상에 선언된 텐서를 GPU로 복사 \n",
    "x1 = torch.FloatTensor(2,2)\n",
    "print(x1)\n",
    "x2 = x1.cuda()\n",
    "print(x2)\n",
    "\n",
    "print('='*100)\n",
    "\n",
    "# nn.Module 의 하위 클래스 객체에 적용 \n",
    "import torch.nn as nn\n",
    "layer = nn.Linear(2, 2)\n",
    "layer.cuda(0)\n",
    "\n",
    "### <주의>\n",
    "### 텐서는 cuda 함수를 통해 원하는 디바이스로 복사가 되지만, \n",
    "### nn.Module 하위 클래스 객체는 \"복사\"가 아닌 \"이동\"이 된다"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU 사용하기 - 서로 다른 장치 간 연산 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mFloatTensor(\u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[0;32m      2\u001b[0m x\n\u001b[1;32m----> 4\u001b[0m \u001b[39mprint\u001b[39m(x \u001b[39m+\u001b[39;49m x\u001b[39m.\u001b[39;49mcuda(\u001b[39m0\u001b[39;49m))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "# 서로 다른 장치에 올라가 있는 텐서/nn.Module의 하위 클래스 객체끼리는 연산 불가 \n",
    "# CPU와 GPU에 위치한 텐서들끼리도 연산 불가 \n",
    "# GPU 0번과 GPU 1번 사이도 연산 불가 \n",
    "\n",
    "x = torch.FloatTensor(2, 2)\n",
    "x\n",
    "\n",
    "print(x + x.cuda(0))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU 사용하기 - CPU 함수 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 0.]], device='cuda:0')\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# GPU 메모리에 있는 텐서를 CPU 메모리로 복사해야 할 때 \n",
    "x = torch.cuda.FloatTensor(2, 2) \n",
    "print(x)\n",
    "\n",
    "x1 = x.cpu()\n",
    "print(x1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU 사용하기 - To함수 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[       nan, 0.0000e+00],\n",
      "        [5.6052e-45, 0.0000e+00]])\n",
      "tensor([[       nan, 0.0000e+00],\n",
      "        [5.6052e-45, 0.0000e+00]], device='cuda:0')\n",
      "tensor([[       nan, 0.0000e+00],\n",
      "        [5.6052e-45, 0.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "cpu_device = torch.device('cpu')\n",
    "gpu_device = torch.device('cuda:0')\n",
    "\n",
    "# CPU\n",
    "x = torch.FloatTensor(2, 2)\n",
    "print(x)\n",
    "\n",
    "# CPU -> GPU \n",
    "x1 = x.to(gpu_device)\n",
    "print(x1)\n",
    "\n",
    "# GPU -> CPU \n",
    "x2 = x1.to(cpu_device)\n",
    "print(x2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU 사용하기 - Device 속성 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# 텐서는 device 속성을 가지고 있어, 해당 텐서가 위치한 디바이스를 쉽게 파악 가능 \n",
    "x = torch.cuda.FloatTensor(2, 2)\n",
    "print(x.device)\n",
    "\n",
    "# But, nn.Module의 하위 클래스 객체는 device 속성이 없다. \n",
    "# So, 아래와 같은 방법을 사용한다.\n",
    "\n",
    "layer = nn.Linear(2, 2)\n",
    "print(next(layer.parameters()).device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.5000)\n"
     ]
    }
   ],
   "source": [
    "# MSE Loss 구현 \n",
    "def mse(x_hat, x):\n",
    "    # |x_hat| = (batch_size, dim)\n",
    "    # |x| = (batch_size, dim)\n",
    "    y = ((x-x_hat)**2).mean()\n",
    "\n",
    "    return y\n",
    "\n",
    "x = torch.FloatTensor([[1,1],\n",
    "                       [2,2]])\n",
    "x_hat = torch.FloatTensor([[0,0],\n",
    "                           [0,0]])\n",
    "\n",
    "print(mse(x_hat, x))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.nn.functional 사용하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.5000)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 파이토치 내장 MSE 손실 함수 활용 \n",
    "import torch.nn.functional as F\n",
    "F.mse_loss(x_hat, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.)\n",
      "tensor([[1., 1.],\n",
      "        [4., 4.]])\n"
     ]
    }
   ],
   "source": [
    "print(F.mse_loss(x_hat, x, reduction='sum'))\n",
    "print(F.mse_loss(x_hat, x, reduction='none'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.nn 사용하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.5000)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "mse_loss = nn.MSELoss()\n",
    "mse_loss(x_hat, x)      \n",
    "\n",
    "### torch.nn.functional 과 torch.nn 의 차이는 거의 없다 \n",
    "### But, torch.nn은 nn.Module의 하위 클래스 내부에 선언하기에, 레이어의 하나처럼 취급 가능함"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 경사하강법 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.FloatTensor([[.1, .2, .3],\n",
    "                            [.4, .5, .6],\n",
    "                            [.7, .8, .9]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3744, 0.9964, 0.2138],\n",
       "        [0.8847, 0.1022, 0.0541],\n",
       "        [0.6321, 0.2298, 0.8967]], requires_grad=True)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand_like(target)\n",
    "# This means the final scalar will be differentiate by x.\n",
    "x.requires_grad = True\n",
    "# You can get gradient of x, after differentiation.\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1931, grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = F.mse_loss(x, target)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-th Loss: 1.1682e-01\n",
      "tensor([[0.3134, 0.8195, 0.2330],\n",
      "        [0.7770, 0.1906, 0.1754],\n",
      "        [0.6472, 0.3565, 0.8974]], requires_grad=True)\n",
      "2-th Loss: 7.0671e-02\n",
      "tensor([[0.2660, 0.6818, 0.2479],\n",
      "        [0.6932, 0.2593, 0.2697],\n",
      "        [0.6589, 0.4550, 0.8980]], requires_grad=True)\n",
      "3-th Loss: 4.2752e-02\n",
      "tensor([[0.2291, 0.5747, 0.2595],\n",
      "        [0.6280, 0.3128, 0.3431],\n",
      "        [0.6681, 0.5317, 0.8985]], requires_grad=True)\n",
      "4-th Loss: 2.5862e-02\n",
      "tensor([[0.2004, 0.4915, 0.2685],\n",
      "        [0.5774, 0.3544, 0.4002],\n",
      "        [0.6752, 0.5913, 0.8988]], requires_grad=True)\n",
      "5-th Loss: 1.5645e-02\n",
      "tensor([[0.1781, 0.4267, 0.2755],\n",
      "        [0.5380, 0.3868, 0.4446],\n",
      "        [0.6807, 0.6377, 0.8991]], requires_grad=True)\n",
      "6-th Loss: 9.4642e-03\n",
      "tensor([[0.1607, 0.3763, 0.2809],\n",
      "        [0.5073, 0.4119, 0.4791],\n",
      "        [0.6850, 0.6738, 0.8993]], requires_grad=True)\n",
      "7-th Loss: 5.7253e-03\n",
      "tensor([[0.1472, 0.3371, 0.2852],\n",
      "        [0.4835, 0.4315, 0.5060],\n",
      "        [0.6883, 0.7018, 0.8994]], requires_grad=True)\n",
      "8-th Loss: 3.4634e-03\n",
      "tensor([[0.1367, 0.3067, 0.2885],\n",
      "        [0.4649, 0.4467, 0.5269],\n",
      "        [0.6909, 0.7236, 0.8996]], requires_grad=True)\n",
      "9-th Loss: 2.0952e-03\n",
      "tensor([[0.1286, 0.2830, 0.2910],\n",
      "        [0.4505, 0.4586, 0.5431],\n",
      "        [0.6929, 0.7406, 0.8997]], requires_grad=True)\n",
      "10-th Loss: 1.2674e-03\n",
      "tensor([[0.1222, 0.2645, 0.2930],\n",
      "        [0.4393, 0.4678, 0.5558],\n",
      "        [0.6945, 0.7538, 0.8997]], requires_grad=True)\n",
      "11-th Loss: 7.6673e-04\n",
      "tensor([[0.1173, 0.2502, 0.2946],\n",
      "        [0.4305, 0.4749, 0.5656],\n",
      "        [0.6957, 0.7641, 0.8998]], requires_grad=True)\n",
      "12-th Loss: 4.6382e-04\n",
      "tensor([[0.1134, 0.2390, 0.2958],\n",
      "        [0.4238, 0.4805, 0.5732],\n",
      "        [0.6967, 0.7721, 0.8998]], requires_grad=True)\n",
      "13-th Loss: 2.8058e-04\n",
      "tensor([[0.1105, 0.2304, 0.2967],\n",
      "        [0.4185, 0.4848, 0.5792],\n",
      "        [0.6974, 0.7783, 0.8999]], requires_grad=True)\n",
      "14-th Loss: 1.6974e-04\n",
      "tensor([[0.1081, 0.2236, 0.2974],\n",
      "        [0.4144, 0.4882, 0.5838],\n",
      "        [0.6980, 0.7831, 0.8999]], requires_grad=True)\n",
      "15-th Loss: 1.0268e-04\n",
      "tensor([[0.1063, 0.2184, 0.2980],\n",
      "        [0.4112, 0.4908, 0.5874],\n",
      "        [0.6984, 0.7869, 0.8999]], requires_grad=True)\n",
      "16-th Loss: 6.2115e-05\n",
      "tensor([[0.1049, 0.2143, 0.2985],\n",
      "        [0.4087, 0.4929, 0.5902],\n",
      "        [0.6988, 0.7898, 0.8999]], requires_grad=True)\n",
      "17-th Loss: 3.7576e-05\n",
      "tensor([[0.1038, 0.2111, 0.2988],\n",
      "        [0.4068, 0.4945, 0.5924],\n",
      "        [0.6991, 0.7920, 0.9000]], requires_grad=True)\n",
      "18-th Loss: 2.2731e-05\n",
      "tensor([[0.1030, 0.2086, 0.2991],\n",
      "        [0.4053, 0.4957, 0.5941],\n",
      "        [0.6993, 0.7938, 0.9000]], requires_grad=True)\n",
      "19-th Loss: 1.3751e-05\n",
      "tensor([[0.1023, 0.2067, 0.2993],\n",
      "        [0.4041, 0.4966, 0.5954],\n",
      "        [0.6994, 0.7952, 0.9000]], requires_grad=True)\n",
      "20-th Loss: 8.3184e-06\n",
      "tensor([[0.1018, 0.2052, 0.2994],\n",
      "        [0.4032, 0.4974, 0.5964],\n",
      "        [0.6996, 0.7963, 0.9000]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "threshold = 1e-5\n",
    "learning_rate = 1.\n",
    "iter_cnt = 0\n",
    "\n",
    "while loss > threshold:\n",
    "    iter_cnt += 1\n",
    "    \n",
    "    loss.backward() # Calculate gradients.\n",
    "\n",
    "    x = x - learning_rate * x.grad\n",
    "    \n",
    "    # You don't need to aware this now.\n",
    "    x.detach_()\n",
    "    x.requires_grad_(True)\n",
    "    \n",
    "    loss = F.mse_loss(x, target)\n",
    "    \n",
    "    print('%d-th Loss: %.4e' % (iter_cnt, loss))\n",
    "    print(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파이토치 오토그래드 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 4.],\n",
      "        [5., 6.]], grad_fn=<AddBackward0>)\n",
      "tensor([[-1.,  0.],\n",
      "        [ 1.,  2.]], grad_fn=<SubBackward0>)\n",
      "tensor([[-3.,  0.],\n",
      "        [ 5., 12.]], grad_fn=<MulBackward0>)\n",
      "tensor(14., grad_fn=<SumBackward0>)\n",
      "x.grad : tensor([[2., 4.],\n",
      "        [6., 8.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-3.,  0.],\n",
       "        [ 5., 12.]])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# requires_grad=True 인 텐서는 연산 결과가 담긴 텐서도 속성값이 True를 갖게됨 \n",
    "x = torch.FloatTensor([[1,2],\n",
    "                       [3,4]]).requires_grad_(True)\n",
    "\n",
    "x1 = x+2\n",
    "print(x1)\n",
    "\n",
    "x2 = x-2\n",
    "print(x2)\n",
    "\n",
    "x3 = x1*x2\n",
    "print(x3)\n",
    "\n",
    "y = x3.sum()\n",
    "print(y)\n",
    "\n",
    "y.backward() # 스칼라 값에 backward 함수를 호출하여 자동 미분을 수행\n",
    "\n",
    "print(f'x.grad : {x.grad}')\n",
    "\n",
    "x3.detach_() # 텐서를 메모리에서 해제 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8d53016de1ba688087103bdb1d336e55aee453f16599935fdd23fb9d497997ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
